{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":307,"outputs":[{"output_type":"stream","text":"/kaggle/input/ee-769-assignment1/train.csv\n/kaggle/input/ee-769-assignment1/test.csv\n/kaggle/input/ee-769-assignment1/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing some important packages\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport csv as csv\n%matplotlib inline\n","execution_count":308,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the dataset\ntrain = pd.read_csv('../input/ee-769-assignment1/train.csv')\ntest = pd.read_csv('../input/ee-769-assignment1/test.csv')\nprint('-'*30)\nprint('size of training set: {}'.format(train.shape))\nprint('size of testing set: {}'.format(test.shape))\nprint('-'*30)\n\n# print('sample of the training set \\n {}'.format(train.head()))\n# print('sample of the test set \\n {}'.format(test.head()))","execution_count":309,"outputs":[{"output_type":"stream","text":"------------------------------\nsize of training set: (1028, 34)\nsize of testing set: (441, 33)\n------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the columns' names to decide the data type of each column numerical/categorical\ntrain.columns\n","execution_count":310,"outputs":[{"output_type":"execute_result","execution_count":310,"data":{"text/plain":"Index(['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n       'DistanceFromHome', 'Education', 'EducationField', 'EmployeeCount',\n       'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate',\n       'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n       'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n       'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n       'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n       'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n       'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n       'ID'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":311,"outputs":[{"output_type":"execute_result","execution_count":311,"data":{"text/plain":"Index(['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome',\n       'Education', 'EducationField', 'EmployeeCount', 'EmployeeNumber',\n       'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement',\n       'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus',\n       'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime',\n       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager', 'ID'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the ID column of the test file \nID= test.columns[-1]\nID_test = np.array(test.ID)\n#print(ID_test)\n#print(type(ID_test))","execution_count":312,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to figure out the type of each column\nprint(train.dtypes)","execution_count":313,"outputs":[{"output_type":"stream","text":"Age                          int64\nAttrition                    int64\nBusinessTravel              object\nDailyRate                    int64\nDepartment                  object\nDistanceFromHome             int64\nEducation                    int64\nEducationField              object\nEmployeeCount                int64\nEmployeeNumber               int64\nEnvironmentSatisfaction      int64\nGender                      object\nHourlyRate                   int64\nJobInvolvement               int64\nJobLevel                     int64\nJobRole                     object\nJobSatisfaction              int64\nMaritalStatus               object\nMonthlyIncome                int64\nMonthlyRate                  int64\nNumCompaniesWorked           int64\nOverTime                    object\nPercentSalaryHike            int64\nPerformanceRating            int64\nRelationshipSatisfaction     int64\nStockOptionLevel             int64\nTotalWorkingYears            int64\nTrainingTimesLastYear        int64\nWorkLifeBalance              int64\nYearsAtCompany               int64\nYearsInCurrentRole           int64\nYearsSinceLastPromotion      int64\nYearsWithCurrManager         int64\nID                           int64\ndtype: object\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = ['BusinessTravel', 'Department', 'EducationField','Gender', \n                       'JobRole','MaritalStatus', 'OverTime']\nnumerical_columns = ['Age', 'DailyRate','DistanceFromHome', 'Education', 'EnvironmentSatisfaction',\n                     'HourlyRate','JobInvolvement', 'JobLevel', 'JobSatisfaction',\n                     'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n                     'PercentSalaryHike', 'PerformanceRating',\n                     'RelationshipSatisfaction', 'StockOptionLevel',\n                     'TotalWorkingYears','TrainingTimesLastYear', \n                     'WorkLifeBalance', 'YearsAtCompany','YearsInCurrentRole',\n                     'YearsSinceLastPromotion', 'YearsWithCurrManager']#,'ID','EmployeeCount','EmployeeNumber'\noutputs = ['Attrition']\n# Dropping the irrelevant columns\n#train=train.drop(['Attrition'],axis=1)\ntrain=train.drop(['ID'],axis=1)\ntrain=train.drop(['EmployeeCount'],axis=1)\ntrain=train.drop(['EmployeeNumber'],axis=1)\ntest=test.drop(['ID'],axis=1)\ntest=test.drop(['EmployeeCount'],axis=1)\ntest=test.drop(['EmployeeNumber'],axis=1)","execution_count":314,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the categorical and the numerical training data into tensors\nfor category in categorical_columns:\n    train[category] = train[category].astype('category')\n    \nBusinessTravel = train['BusinessTravel'].cat.codes.values\nDepartment = train['Department'].cat.codes.values\nEducationField = train['EducationField'].cat.codes.values\nGender = train['Gender'].cat.codes.values\nJobRole = train['JobRole'].cat.codes.values\nMaritalStatus = train['MaritalStatus'].cat.codes.values\nOverTime = train['OverTime'].cat.codes.values\n\ncategorical_data = np.stack([BusinessTravel, Department, EducationField, Gender, \n                       JobRole, MaritalStatus, OverTime],1)\n\n# convert the categorical data into a tensor\n\ncategorical_data = torch.tensor(categorical_data, dtype = torch.int64)\n# implementing One-hot encoding increases the accuracy\ncategorical_data = torch.nn.functional.one_hot(categorical_data)\n\n\nprint('type of the categorical data \\n {}'.format(type(categorical_data)))\nprint('shape of the categorical data \\n {}'.format(categorical_data.shape))\nprint('sample of the categorical data after converting to tensor \\n {}'.format(categorical_data[:5]))\n\nnumerical_data = np.stack([train[col].values for col in numerical_columns],1)\nnumerical_data = torch.tensor(numerical_data,dtype = torch.int64)#torch.long)# dtype = torch.float)\nprint('-*'*30)\nprint('type of the numerical data \\n {}'.format(type(numerical_data)))\nprint('shape of the numerical data \\n {}'.format(numerical_data.shape))\nprint('sample of the numerical data after converting to tensor \\n {}'.format(numerical_data[:2]))\noutputs = torch.tensor(train[outputs].values)\nprint('-*'*30)\nprint('output type :{}'.format(type(outputs)))\nprint('output shape :{}'.format(outputs.shape))\nprint(outputs[:5])\ncategorical_data=categorical_data.view(categorical_data.shape[0],categorical_data.shape[1]*categorical_data.shape[2])\nprint(categorical_data.shape)","execution_count":315,"outputs":[{"output_type":"stream","text":"type of the categorical data \n <class 'torch.Tensor'>\nshape of the categorical data \n torch.Size([1028, 7, 9])\nsample of the categorical data after converting to tensor \n tensor([[[0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 1, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 1, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 1, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 1, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 1, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0]]])\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\ntype of the numerical data \n <class 'torch.Tensor'>\nshape of the numerical data \n torch.Size([1028, 23])\nsample of the numerical data after converting to tensor \n tensor([[   41,  1102,     1,     2,     2,    94,     3,     2,     4,  5993,\n         19479,     8,    11,     3,     1,     0,     8,     0,     1,     6,\n             4,     0,     5],\n        [   49,   279,     8,     1,     3,    61,     2,     2,     2,  5130,\n         24907,     1,    23,     4,     4,     1,    10,     3,     3,    10,\n             7,     1,     7]])\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\noutput type :<class 'torch.Tensor'>\noutput shape :torch.Size([1028, 1])\ntensor([[1],\n        [0],\n        [1],\n        [0],\n        [0]])\ntorch.Size([1028, 63])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ######################################Normalization##################################################\n# # calculate column means\n# from math import sqrt\n# def column_means(dataset):\n#     means = [0 for i in range(len(dataset[0]))]\n#     for i in range(len(dataset[0])):\n#         col_values = [row[i] for row in dataset]\n#         means[i] = sum(col_values) / float(len(dataset))\n#     return means\n \n# # calculate column standard deviations\n# def column_stdevs(dataset, means):\n#     stdevs = [0 for i in range(len(dataset[0]))]\n#     for i in range(len(dataset[0])):\n#         variance = [pow(row[i]-means[i], 2) for row in dataset]\n#         stdevs[i] = sum(variance)\n#     stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]\n#     return stdevs\n \n# # standardize dataset\n# def standardize_dataset(dataset, means, stdevs):\n#     for row in dataset:\n#         for i in range(len(row)):\n#             row[i] = (row[i] - means[i]) / stdevs[i]\n#     return dataset\n \n# # Estimate mean and standard deviation\n# means = column_means(numerical_data)\n# stdevs = column_stdevs(numerical_data, means)\n# print(means)\n# print(stdevs)\n# # standardize dataset\n# numerical_data = standardize_dataset(numerical_data, means, stdevs)\n# numerical_data = torch.tensor(numerical_data)","execution_count":316,"outputs":[{"output_type":"stream","text":"[tensor(36.9990), tensor(806.5516), tensor(9.0107), tensor(2.8735), tensor(2.7198), tensor(65.4514), tensor(2.7315), tensor(2.0934), tensor(2.7578), tensor(6632.5737), tensor(14243.5312), tensor(2.6975), tensor(15.1722), tensor(3.1527), tensor(2.7325), tensor(0.7588), tensor(11.4173), tensor(2.7695), tensor(2.7636), tensor(7.1002), tensor(4.2636), tensor(2.2101), tensor(4.1411)]\n[9.44429953470197, 407.0436670370883, 8.078419191179545, 1.0328392152317654, 1.0896158834716911, 20.27423228210889, 0.7032395587421315, 1.141851680039347, 1.1053034251031704, 4855.748551974248, 7048.767551849047, 2.5275294900413257, 3.647639250577825, 0.3598954330015187, 1.0887695816153624, 0.8362364189972453, 8.015445104889197, 1.302515700716071, 0.7030036185715445, 6.316289085570736, 3.630384720941556, 3.288566787586723, 3.60846081010031]\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate the numerical and the catigorical data in one tensor to form the final data set\ndata = torch.cat([categorical_data,  numerical_data], dim=1)\n#data = np.array(data)\nx = data.numpy()\nprint('type of the data{}'.format(type(x)))\nprint('size of the data{}'.format(x.shape))\ny = outputs.numpy()\nprint('type of the output{}'.format(type(y)))\nprint('size of the output{}'.format(y.shape))","execution_count":317,"outputs":[{"output_type":"stream","text":"type of the data<class 'numpy.ndarray'>\nsize of the data(1028, 86)\ntype of the output<class 'numpy.ndarray'>\nsize of the output(1028, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the categorical and the numerical testing data into tensors\nfor category in categorical_columns:\n    test[category] = test[category].astype('category')\n    \nBusinessTravel_t = test['BusinessTravel'].cat.codes.values\nDepartment_t = test['Department'].cat.codes.values\nEducationField_t = test['EducationField'].cat.codes.values\nGender_t = test['Gender'].cat.codes.values\nJobRole_t = test['JobRole'].cat.codes.values\nMaritalStatus_t = test['MaritalStatus'].cat.codes.values\nOverTime_t = test['OverTime'].cat.codes.values\n\ncategorical_data_t = np.stack([BusinessTravel_t, Department_t, EducationField_t, Gender_t, \n                       JobRole_t, MaritalStatus_t, OverTime_t],1)\n\n# convert the categorical data into a tensor\n\ncategorical_data_t = torch.tensor(categorical_data_t, dtype = torch.int64)\ncategorical_data_t = torch.nn.functional.one_hot(categorical_data_t)\n#categorical_data_t = torch.nn.functional.embedding(categorical_data_t)\n\nprint('type of the categorical data \\n {}'.format(type(categorical_data_t)))\nprint('shape of the categorical data \\n {}'.format(categorical_data_t.shape))\nprint('sample of the categorical data after converting to tensor \\n {}'.format(categorical_data_t[:5]))\n\nnumerical_data_t = np.stack([test[col].values for col in numerical_columns],1)\nnumerical_data_t = torch.tensor(numerical_data_t,dtype = torch.int64)#torch.long)# dtype = torch.float)\nprint('-*'*30)\nprint('type of the numerical data \\n {}'.format(type(numerical_data_t)))\nprint('shape of the numerical data \\n {}'.format(numerical_data_t.shape))\nprint('sample of the numerical data after converting to tensor \\n {}'.format(numerical_data_t[:2]))\ncategorical_data_t=categorical_data_t.view(categorical_data_t.shape[0],categorical_data_t.shape[1]*categorical_data_t.shape[2])\n","execution_count":318,"outputs":[{"output_type":"stream","text":"type of the categorical data \n <class 'torch.Tensor'>\nshape of the categorical data \n torch.Size([441, 7, 9])\nsample of the categorical data after converting to tensor \n tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 1, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 1, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 1, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0]],\n\n        [[0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 1, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 0, 0]]])\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\ntype of the numerical data \n <class 'torch.Tensor'>\nshape of the numerical data \n torch.Size([441, 23])\nsample of the numerical data after converting to tensor \n tensor([[   40,   663,     9,     4,     3,    81,     3,     2,     3,  3975,\n         23099,     3,    11,     3,     3,     2,    11,     2,     4,     8,\n             7,     0,     7],\n        [   31,   326,     8,     2,     1,    31,     3,     3,     4, 10793,\n          8386,     1,    18,     3,     1,     1,    13,     5,     3,    13,\n             7,     9,     9]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# means = column_means(numerical_data_t)\n# stdevs = column_stdevs(numerical_data_t, means)\n# print(means)\n# print(stdevs)\n# # standardize dataset\n# numerical_data_t = standardize_dataset(numerical_data_t, means, stdevs)\n# numerical_data_t = torch.tensor(numerical_data_t)","execution_count":319,"outputs":[{"output_type":"stream","text":"[tensor(36.7392), tensor(791.9184), tensor(9.6259), tensor(3.), tensor(2.7279), tensor(66.8617), tensor(2.7234), tensor(1.9977), tensor(2.6599), tensor(6210.6484), tensor(14495.1270), tensor(2.6848), tensor(15.3039), tensor(3.1565), tensor(2.6689), tensor(0.8776), tensor(10.9683), tensor(2.8639), tensor(2.7574), tensor(6.8005), tensor(4.1542), tensor(2.1406), tensor(4.0839)]\n[8.38763340305261, 395.2178987596589, 8.172474826366715, 0.995444172005069, 1.1030385294191303, 20.437187915717363, 0.7296480678710785, 1.0191330406108525, 1.0964608521855062, 4335.68056941468, 7277.137074427003, 2.4332879671893153, 3.691907802195183, 0.36370539771928984, 1.0613724953057262, 0.8834162985029036, 7.209771191144842, 1.2539366246047228, 0.7151576535938534, 5.666575075793103, 3.611697684029996, 3.0678980994777674, 3.4797755791659704]\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenating the numerical and the catigorical data in one tensor to form the final test set\ndata_t = torch.cat([categorical_data_t,  numerical_data_t], dim=1)\n#data = np.array(data)\nx_test = data_t.numpy()\nprint('type of the test data{}'.format(type(x_test)))\nprint('size of the test data{}'.format(x_test.shape))\n","execution_count":320,"outputs":[{"output_type":"stream","text":"type of the test data<class 'numpy.ndarray'>\nsize of the test data(441, 86)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementing different classifiers\n# import the important packages\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics","execution_count":321,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dividing the dataset into training and validation datasets\n# I tried many values of the size of validation set, the value 20% gives the best results\nx_train, x_val, y_train, y_val = train_test_split(x,y, test_size= 0.2, random_state=42,shuffle=True)\nprint('the size of the training set{}'.format(x_train.shape))\nprint('the size of the validation set{}'.format(x_val.shape))","execution_count":322,"outputs":[{"output_type":"stream","text":"the size of the training set(822, 86)\nthe size of the validation set(206, 86)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Cross Validation\n# from sklearn.model_selection import KFold \n# n_splits = 5\n# kf = KFold(n_splits=5, random_state=42) \n# Total_Acc = 0.0\n# for train_index, test_index in kf.split(x):\n#     #print(\"Train:\", train_index, \"Validation:\",test_index)\n#     x_train, x_val = x[train_index], x[test_index] \n#     y_train, y_val = y[train_index], y[test_index]\n#     clf = AdaBoostClassifier()\n#     clf.fit(x_train, y_train)\n#     y_pred = clf.predict(x_val)\n#     Acc = accuracy_score(y_val, y_pred)\n#     Total_Acc =Total_Acc + Acc\n# print('validation accuracy = {}'.format(Total_Acc/n_splits))","execution_count":323,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building the classifier\n# Deep Learning------It gave very low accuracy I guess because the training dataset size is small ---val_acc=0.85\n# clf = MLPClassifier(hidden_layer_sizes=(10,5,3), activation='relu', solver='adam', alpha=0.3, batch_size=200, learning_rate=  'adaptive', learning_rate_init=0.01\n#                     , max_iter=200, shuffle=True, random_state=42, momentum=0.9, nesterovs_momentum=True,\n#                     early_stopping=True, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=100,verbose=False)# accuracy = 0.88\n\n# # Random Forest Classifier\n# clf = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n#                              max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None,\n#                               verbose=1, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)# accuracy = 0.88\n\n# #Support Vector Machine-----I tried various values of the parameters but the accuracy was low for all\n# clf = svm.SVC(C=0.01, kernel='rbf', degree=3, gamma=0.1, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False,\n#               max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None) # accuracy = 0.85\n\n# rng = np.random.RandomState(1)\n#clf = AdaBoostClassifier(RandomForestClassifier(n_estimators = 1000, max_depth = 3),n_estimators=10000, random_state=47)\n\n## I got the best results for AdaBoost classifier with the default values of the free parameters, \n## this ensemble classifier combines multiple classifiers to increase the accuracy of classifiers\n## It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability for classification.\n## Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The more accurate classifier will get high weight.\n## This process iterate until the complete training data fits without any error or until reached to the specified maximum number of estimators.\nclf = AdaBoostClassifier() #accuracy = 0.91414\n\n#clf = GradientBoostingClassifier()# accuracy= 0.859\n\n#clf = HistGradientBoostingClassifier()# accuracy = 0.854\n\n# svc=svm.SVC(probability=True, kernel='rbf')\n# clf = AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1)# accuracy = 0.85\n\n# clf = GaussianNB() #accuracy = 0.37\n\n\n# for ploy\n# clf = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.01, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False,\n#               max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n\n# # Decision Tree Classifier\n# clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n#                              max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n#                              class_weight=None, presort='deprecated', ccp_alpha=0.0) #accuracy = 0.83\n","execution_count":324,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training and testing the classifier\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)\nprint('the prediction array:\\n {}'.format(y_pred))","execution_count":325,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"},{"output_type":"stream","text":"the prediction array:\n [0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# computing the validation accuracy\naccuracy_score(y_val, y_pred)\n#clf.score(x_val, y_val)","execution_count":326,"outputs":[{"output_type":"execute_result","execution_count":326,"data":{"text/plain":"0.8786407766990292"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# computing the confusion matrix\ncm = confusion_matrix(y_val, y_pred)\ncm","execution_count":327,"outputs":[{"output_type":"execute_result","execution_count":327,"data":{"text/plain":"array([[168,   9],\n       [ 16,  13]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm, center=True)\nplt.show()","execution_count":328,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction of the test data set\ntest_pred = clf.predict(x_test)\nprint('the results on the testing data set\\n{}'.format(test_pred))\nprint(' the IDs of the test file')","execution_count":329,"outputs":[{"output_type":"stream","text":"the results on the testing data set\n[0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n the IDs of the test file\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the csv file for submission\nimport itertools\n\nFile = np.stack((ID_test, test_pred),axis= 1)\nresult = pd.DataFrame({'ID': File[:, 0], 'Attrition': File[:, 1]})\nresult.to_csv('myResult.csv',index=False)\n\n\n","execution_count":330,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}