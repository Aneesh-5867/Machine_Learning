{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_with_AWS_DeepComposer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SudhakarKuma/Machine_Learning/blob/master/Udacity/AWS_Machine_Learning_Course/Machine_Learning_with_AWS_DeepComposer/Machine_Learning_with_AWS_DeepComposer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIUOoqAoeku",
        "colab_type": "text"
      },
      "source": [
        "# ML Techniques and Generative AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VWopgxpoi6V",
        "colab_type": "text"
      },
      "source": [
        "**Machine Learning Techniques**\n",
        "\n",
        "* **Supervised Learning**: Models are presented with input data and the desired results. The model will then attempt to learn rules that map the input data to the desired results.\n",
        "* **Unsupervised Learning**: Models are presented with datasets that have no labels or predefined patterns, and the model will attempt to infer the underlying structures from the dataset. **Generative AI** is a type of unsupervised learning.\n",
        "* **Reinforcement learning**: The model or agent will interact with a dynamic world to achieve a certain goal. The dynamic world will reward or punish the agent based on its actions. Overtime, the agent will learn to navigate the dynamic world and accomplish its goal(s) based on the rewards and punishments that it has received.\n",
        "\n",
        "![ML techniques](https://video.udacity-data.com/topher/2020/May/5eb1c99a_types-of-ml/types-of-ml.jpg)\n",
        "\n",
        "\n",
        "**Generative AI**\n",
        "\n",
        "Generative AI is one of the biggest recent advancements in artificial intelligence technology because of its ability to create something new. It opens the door to an entire world of possibilities for human and computer creativity, with practical applications emerging across industries, from turning sketches into images for accelerated product development, to improving computer-aided design of complex objects. It takes two neural networks against each other to produce new and original digital works based on sample inputs.\n",
        "\n",
        "![Generative AI](https://video.udacity-data.com/topher/2020/May/5eb1cbb0_generative-ai/generative-ai.jpg)\n",
        "\n",
        "**Additional Resources**\n",
        "\n",
        "* [AWS DeepLens](https://aws.amazon.com/deeplens/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0vvxo2PrG5B",
        "colab_type": "text"
      },
      "source": [
        "# AWS DeepComposer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpdaka04rIX9",
        "colab_type": "text"
      },
      "source": [
        "AWS Deep Composer uses Generative AI, or specifically Generative Adversarial Networks (GANs), to generate music. GANs pit 2 networks, a generator and a discriminator, against each other to generate new content.\n",
        "\n",
        "The best way we’ve found to explain this is to use the metaphor of an orchestra and conductor. In this context, the generator is like the orchestra and the discriminator is like the conductor. The orchestra plays and generates the music. The conductor judges the music created by the orchestra and coaches the orchestra to improve for future iterations. So an orchestra, trains, practices, and tries to generate music, and then the conductor coaches them to produced more polished music.\n",
        "\n",
        "GANS is Similar to an Orchestra and a Conductor: The More They Work Together, the Better They Can Perform!\n",
        "![GAN](https://video.udacity-data.com/topher/2020/May/5eb1e389_aws-mle-orchestra-metaphor/aws-mle-orchestra-metaphor.jpg)\n",
        "\n",
        "**AWS DeepComposer Workflow**\n",
        "\n",
        "1.   Use the AWS DeepComposer keyboard or play the virtual keyboard in the AWS DeepComposer console to input a melody.\n",
        "\n",
        "2.   Use a model in the AWS DeepComposer console to generate an original musical composition. You can choose from jazz, rock, pop, symphony or Jonathan Coulton pre-trained models or you can also build your own custom genre model in Amazon SageMaker.\n",
        "\n",
        "3.   Publish your tracks to SoundCloud or export MIDI files to your favorite Digital Audio Workstation (like Garage Band) and get even more creative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpoJ9dSqu8x6",
        "colab_type": "text"
      },
      "source": [
        "# How DeepComposer Works\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfnf8kb_u-IM",
        "colab_type": "text"
      },
      "source": [
        "**AWS DeepComposer uses a GAN**\n",
        "\n",
        "![GAN](https://video.udacity-data.com/topher/2020/May/5eb23921_aws-mle-gan-model/aws-mle-gan-model.jpg)\n",
        "\n",
        "Each iteration of the training cycle is called an epoch. The model is trained for thousands of epochs.\n",
        "\n",
        "**Loss Functions**\n",
        "\n",
        "In machine learning, the goal of iterating and completing epochs is to improve the output or prediction of the model. Any output that deviates from the ground truth is referred to as an error. The measure of an error, given a set of weights, is called a loss function. Weights represent how important an associated feature is to determining the accuracy of a prediction, and loss functions are used to update the weights after every iteration. Ideally, as the weights update, the model improves making less and less errors. Convergence happens once the loss functions stabilize.\n",
        "\n",
        "We use loss functions to measure how closely the output from the GAN models match the desired outcome. Or, in the case of DeepComposer, how well does DeepComposer's output music match the training music. Once the loss functions from the Generator and Discriminator converges, this indicates the GAN model is no longer learning, and we can stop its training.\n",
        "\n",
        "We also measures the quality of the music generated by DeepComposer via additional quantitative metrics, such as drum pattern and polyphonic rate.\n",
        "\n",
        "![loss](https://video.udacity-data.com/topher/2020/May/5eae0e21_screen-shot-2020-05-02-at-7.18.44-pm/screen-shot-2020-05-02-at-7.18.44-pm.png)\n",
        "\n",
        "GAN loss functions have many fluctuations early on due to the \"adversarial\" nature of the generator and discriminator.\n",
        "\n",
        "Over time, the loss functions stabilizes to a point, we call this convergence. This convergence can be zero, but doesn't have to be.\n",
        "\n",
        "**AWS DeepComposer Under The Hood**\n",
        "![](https://video.udacity-data.com/topher/2020/May/5ebed725_aws-mle-under-hood-v2/aws-mle-under-hood-v2.jpg)\n",
        "\n",
        "**How It Works**\n",
        "\n",
        "* Input melody captured on the AWS DeepComposer console\n",
        "* Console makes a backend call to AWS DeepComposer APIs that triggers an execution Lambda.\n",
        "* Book-keeping is recorded in Dynamo DB.\n",
        "* The execution Lambda performs an inference query to SageMaker which hosts the model and the training inference container.\n",
        "* The query is run on the Generative AI model.\n",
        "* The model generates a composition.\n",
        "* The generated composition is returned.\n",
        "* The user can hear the composition in the console.\n",
        "* The user can share the composition to SoundCloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLEwEszxXuk",
        "colab_type": "text"
      },
      "source": [
        "# Training Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwZo2-S3xcP6",
        "colab_type": "text"
      },
      "source": [
        "**How to measure the quality of the music we’re generating:**\n",
        "\n",
        "* We can monitor the loss function to make sure the model is converging\n",
        "* We can check the similarity index to see how close is the model to mimicking the style of the data. When the graph of the similarity index smoothes out and becomes less spikey, we can be confident that the model is converging\n",
        "* We can listen to the music created by the generated model to see if it's doing a good job. The musical quality of the model should improve as the number of training epochs increases.\n",
        "\n",
        "**Training architecture**\n",
        "\n",
        "* User launches a training job from the AWS DeepComposer console by selecting hyperparameters and data set filtering tags\n",
        "* The backend consists of an API Layer (API gateway and lambda) write request to DynamoDB\n",
        "* Triggers a lambda function that starts the training workflow\n",
        "* It then uses AWS Step Funcitons to launch the training job on Amazon SageMaker\n",
        "* Status is continually monitored and updated to DynamoDB\n",
        "* The console continues to poll the backend for the status of the training job and update the results live so users can see how the model is learning\n",
        "\n",
        "![Training architecture](https://video.udacity-data.com/topher/2020/May/5eb8249e_aws-mle-train-arch/aws-mle-train-arch.png)\n",
        "\n",
        "**Challenges with GANs**\n",
        "\n",
        "* Clean datasets are hard to obtain\n",
        "* Not all melodies sound good in all genres\n",
        "* Convergence in GAN is tricky – it can be fleeting rather than being a stable state\n",
        "* Complexity in defining meaningful quantitive metrics to measure the quality of music created\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsHGLkf_zcdE",
        "colab_type": "text"
      },
      "source": [
        "# Generative AI Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCDMAW7fze9H",
        "colab_type": "text"
      },
      "source": [
        "Generative AI has been described as one of the most promising advances in AI in the past decade by the MIT Technology Review.\n",
        "\n",
        "Generative AI opens the door to an entire world of creative possibilities with practical applications emerging across industries, from turning sketches into images for accelerated product development, to improving computer-aided design of complex objects.\n",
        "\n",
        "For example, Glidewell Dental is training a generative adversarial network adept at constructing detailed 3D models from images. One network generates images and the second inspects those images. This results in an image that has even more anatomical detail than the original teeth they are replacing.\n",
        "\n",
        "Glidewell Dental is training GPU powered GANs to create dental crown models\n",
        "![](https://video.udacity-data.com/topher/2020/May/5eac7878_dental-example/dental-example.jpeg)\n",
        "\n",
        "Generative AI enables computers to learn the underlying pattern associated with a provided input (image, music, or text), and then they can use that input to generate new content. Examples of Generative AI techniques include Generative Adversarial Networks (GANs), Variational Autoencoders, and Transformers.\n",
        "\n",
        "**What are GANs?**\n",
        "\n",
        "GANs, a generative AI technique, pit 2 networks against each other to generate new content. The algorithm consists of two competing networks: a **generator** and a **discriminator**.\n",
        "\n",
        "A **generator** is a convolutional neural network (CNN) that learns to create new data resembling the source data it was trained on.\n",
        "\n",
        "The **discriminator** is another convolutional neural network (CNN) that is trained to differentiate between real and synthetic data.\n",
        "\n",
        "The generator and the discriminator are trained in alternating cycles such that the generator learns to produce more and more realistic data while the discriminator iteratively gets better at learning to differentiate real data from the newly created data.\n",
        "\n",
        "![](https://video.udacity-data.com/topher/2020/May/5eac7812_aws-deepcomoserr-gan-schema/aws-deepcomoserr-gan-schema.png)\n",
        "\n",
        "**Like the collaboration between an orchestra and its conductor**\n",
        "\n",
        "The best way we’ve found to explain this is to use the metaphor of an orchestra and conductor. An orchestra doesn’t create amazing music the first time they get together. They have a conductor who both judges their output, and coaches them to improve. So an orchestra, trains, practices, and tries to generate polished music, and then the conductor works with them, as both judge and coach.\n",
        "\n",
        "The conductor is both judging the quality of the output (were the right notes played with the right tempo) and at the same time providing feedback and coaching to the orchestra (“strings, more volume! Horns, softer in this part! Everyone, with feeling!”). Specifically to achieve a style that the conductor knows about. So, the more they work together the better the orchestra can perform.\n",
        "\n",
        "The Generative AI that AWS DeepComposer teaches developers about uses a similar concept. We have two machine learning models that work together in order to learn how to generate musical compositions in distinctive styles.\n",
        "\n",
        "As a conductor provides feedback to make an orchestra sound better, a GAN's discriminator gives the generator feedback on how to make its data more realistic\n",
        "![](https://video.udacity-data.com/topher/2020/May/5eac799e_gan-representation/gan-representation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu7a2jwj1jy3",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to U-Net Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTZxVV2F1l8A",
        "colab_type": "text"
      },
      "source": [
        "**Training a machine learning model using a dataset of Bach compositions**\n",
        "\n",
        "AWS DeepComposer uses GANs to create realistic accompaniment tracks. When you provide an input melody, such as twinkle-twinkle little star, using the keyboard U-Net will add three additional piano accompaniment tracks to create a new musical composition.\n",
        "\n",
        "The U-Net architecture uses a publicly available dataset of Bach’s compositions for training the GAN. In AWS DeepComposer, the generator network learns to produce realistic Bach-syle music while the discriminator uses real Bach music to differentiate between real music compositions and newly created ones. \n",
        "\n",
        "**How U-Net based model interprets music**\n",
        "\n",
        "Music is written out as a sequence of human readable notes. Experts have not yet discovered a way to translate the human readable format in such a way that computers can understand it. Modern GAN-based models instead treat music as a series of images, and can therefore leverage existing techniques within the computer vision domain.\n",
        "\n",
        "In AWS DeepComposer, we represent music as a two-dimensional matrix (also referred to as a piano roll) with “time” on the horizontal axis and “pitch” on the vertical axis. You might notice this representation looks similar to an image. A one or zero in any particular cell in this grid indicates if a note was played or not at that time for that pitch.\n",
        "\n",
        "The piano roll format discretizes music into small buckets of time and pitch\n",
        "\n",
        "![](https://video.udacity-data.com/topher/2020/May/5eada095_piano-rolls/piano-rolls.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dewts2OR23FH",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9fIe_IX24sw",
        "colab_type": "text"
      },
      "source": [
        "**Generator**\n",
        "\n",
        "The generator network used in AWS DeepComposer is adapted from the U-Net architecture, a popular convolutional neural network that is used extensively in the computer vision domain. The network consists of an “encoder” that maps the single track music data (represented as piano roll images) to a relatively lower dimensional “latent space“ and a ”decoder“ that maps the latent space back to multi-track music data.\n",
        "\n",
        "Here are the inputs provided to the generator:\n",
        "\n",
        "* Single-track piano roll: A single melody track is provided as the input to the generator.\n",
        "* Noise vector: A latent noise vector is also passed in as an input and this is responsible for ensuring that there is a flavor to each output generated by the generator, even when the same input is provided.\n",
        "\n",
        "Notice that the encoding layers of the generator on the left side and decoder layer on on the right side are connected to create a U-shape, thereby giving the name U-Net to this architecture\n",
        "![](https://video.udacity-data.com/topher/2020/May/5eada1b9_u-net/u-net.png)\n",
        "\n",
        "\n",
        "**Discriminator**\n",
        "\n",
        "The goal of the discriminator is to provide feedback to the generator about how realistic the generated piano rolls are, so that the generator can learn to produce more realistic data. The discriminator provides this feedback by outputting a scalar value that represents how “real” or “fake” a piano roll is.\n",
        "\n",
        "Since the discriminator tries to classify data as “real” or “fake”, it is not very different from commonly used binary classifiers. We use a simple architecture for the critic, composed of four convolutional layers and a dense layer at the end.\n",
        "\n",
        "Discriminators are also referred to as critics because they evaluate the input from the generator and provide feedback.\n",
        "\n",
        "Discriminator network architecture consisting of four convolutional layers and a dense layer\n",
        "![](https://video.udacity-data.com/topher/2020/May/5eada228_discriminator/discriminator.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSzf-ut54g6w",
        "colab_type": "text"
      },
      "source": [
        "# Training Methodology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imSCTCio4jAg",
        "colab_type": "text"
      },
      "source": [
        "During training, the generator and discriminator work in a tight loop as following:\n",
        "\n",
        "**Generator**\n",
        "\n",
        "* The generator takes in a batch of single-track piano rolls (melody) as the input and generates a batch of multi-track piano rolls as the output by adding accompaniments to each of the input music tracks.\n",
        "* The discriminator then takes these generated music tracks and predicts how far it deviates from the real data present in your training dataset.\n",
        "\n",
        "**Discriminator**\n",
        "\n",
        "* This feedback from the discriminator is used by the generator to update its weights. As the generator gets better at creating music accompaniments, it begins fooling the discriminator. So, the discriminator needs to be retrained as well.\n",
        "* Beginning with the discriminator on the first iteration, we alternate between training these two networks until we reach some stop condition (ex: the algorithm has seen the entire dataset a certain number of times).\n",
        "\n",
        "**Finer control of AWS DeepComposer with hyperparameters**\n",
        "\n",
        "As you explore training your own custom model in the AWS DeepComposer console, you will notice you have access to several hyperparameters for finer control of this process. Here are a few details on each to help guide your exploration.\n",
        "\n",
        "**Number of epochs**\n",
        "\n",
        "When the training loop has passed through the entire training dataset once, we call that one epoch. Training for a higher number of epochs will mean your model will take longer to complete its training task, but it may produce better output if it has not yet converged. \n",
        "\n",
        "**Training over more epochs will take longer but can lead to a better sounding musical output**\n",
        "\n",
        "**Model training is a trade-off between the number of epochs (i.e. time) and the quality of sample output.**\n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "The learning rate controls how rapidly the weights and biases of each network are updated during training. A higher learning rate might allow the network to explore a wider set of model weights, but might pass over more optimal weights.\n",
        "\n",
        "**Update ratio**\n",
        "\n",
        "A ratio of the number of times the discriminator is updated per generator training epoch. Updating the discriminator multiple times per generator training epoch is useful because it can improve the discriminators accuracy. Changing this ratio might allow the generator to learn more quickly early-on, but will increase the overall training time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxFy1nSN6-7i",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJPj3ONC7AG-",
        "colab_type": "text"
      },
      "source": [
        "Typically when training any sort of model, it is a standard practice to monitor the value of the loss function throughout the duration of the training. The discriminator loss has been found to correlate well with sample quality. You should expect the discriminator loss to converge to zero and the generator loss to converge to some number which need not be zero. When the loss function plateaus, it is an indicator that the model is no longer learning. At this point, you can stop training the model. \n",
        "\n",
        "**Sample output quality improves with more training**\n",
        "\n",
        "After 400 epochs of training, discriminator loss approaches near zero and the generator converges to a steady-state value. Loss is useful as an evaluation metric since the model will not improve as much or stop improving entirely when the loss plateaus.\n",
        "\n",
        "Sample output at 400 epochs features elements of the training dataset\n",
        "![](https://video.udacity-data.com/topher/2020/May/5eada7ce_qualty-graph/qualty-graph.png)\n",
        "\n",
        "While standard mechanisms exist for evaluating the accuracy of more traditional models like classification or regression, evaluating generative models is an active area of research. Within the domain of music generation, this hard problem is even less well-understood.\n",
        "\n",
        "To address this, we take high-level measurements of our data and show how well our model produces music that aligns with those measurements. If our model produces music which is close to the mean value of these measurements for our training dataset, our music should match the general “shape”. \n",
        "\n",
        "Here are a few such measurements:\n",
        "\n",
        "* Empty bar rate: The ratio of empty bars to total number of bars.\n",
        "* Number of pitches used: A metric that captures the distribution and position of pitches.\n",
        "* In Scale Ratio: Ratio of the number of notes that are in the key of C, which is a common key found in music, to the total number of notes.\n",
        "\n",
        "**Music to your ears**\n",
        "Of course, music is much more complex than a few measurements. It is often important to listen directly to the generated music to better understand changes in model performance. You’ll find this final mechanism available as well, allowing you to listen to the model outputs as it learns.\n",
        "\n",
        "Once training has completed, you may use the model created by the generator network to create new musical compositions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU722U6m7z2s",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tscWA4J71dO",
        "colab_type": "text"
      },
      "source": [
        "Once this model is trained, the generator network alone can be run to generate new accompaniments for a given input melody. If you recall, the model took as input a single-track piano roll representing melody and a noise vector to help generate varied output.\n",
        "\n",
        "The final process for music generation then is as follows:\n",
        "\n",
        "* Transform single-track music input into piano roll format.\n",
        "* Create a series of random numbers to represent the random noise vector.\n",
        "* Pass these as input to our trained generator model, producing a series of output piano rolls. Each output piano roll represents some instrument in the composition.\n",
        "* Transform the series of piano rolls back into a common music format (MIDI), assigning an instrument for each track.\n",
        "\n",
        "Explore Generative AI Further\n",
        "\n",
        "* [Build your own GAN model](https://github.com/aws-samples/aws-deepcomposer-samples)\n"
      ]
    }
  ]
}